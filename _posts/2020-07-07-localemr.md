---
title: A Local EMR
subtitle: Trying to improve the development experience
tags: [aws, python, data engineering, devops]
---

Although cloud computing offers many benefits in terms of maintenance, this 
frequently comes at the cost of local development. This is
evidenced by tools such as [moto][1] and [openstack][2] which attempt to mimick
AWS infrastructure. [Localemr][0] is a service 
you can run on your computer that tries to imitate Amazon EMR. 
With it developers can create 'clusters' and submit Spark jobs locally.

I had a lot of fun building this, and in this post I'll cover what motivated it
and how it currently works, although the latter is definitely subject to change.
The current implementation is tuned to what our requirements are at work.

## On Local Development

Why is being able to develop locally important? Well in a worst-case scenario, 
if CI/CD and staging environments are absent and/or unusable, 
then the only way to test your code is to deploy it to production. Obviously not ideal. 
Now what if you do have CI/CD and staging, but no local development? CI/CD is not binary, 
there are varying levels of quality, and chances are if you cannot provide development environments, 
then you probably don't have robust integration testing in your CI/CD either as these will frequently
use similar infrastructure. So then you're left with staging. Staging becomes the de facto development
environment because it's where developers can actually see if their changes work. The pattern I've witnessed
tends to go something like: 
1. Developers merge a PR to staging not knowing if things will break or not.
2. Something breaks in staging from a merged PR. This is another PR to fix, costing more developer time in review.
3. Repeat steps 1 and 2 until staging becomes unusable from breaking so many times.
4. Test in production.

The difference between a staging environment and a development environment is that 
development environments are _per user_ (or per PR). The developer doesn't need to inconvenience other
devs with a PR to test their changes, they can make the change and test the results themselves
right away and break the environment as they please. Speaking with a colleague, he mentioned 
a company he had worked for didn't even have staging, as their local development experience was
so complete that staging ended up being more of a hindrance than a help.

## Local EMR

My data engineering team makes heavy use of [Amazon EMR][3]. From the site:

> Amazon EMR is a managed cluster platform that simplifies running big data frameworks.

Our use case is mainly to submit [Spark][6] jobs. When a cluster is created, it is
given a [release label][5] which determines the version of Spark used. We schedule
these Spark jobs using [Airflow][4] with the assumption that a long running EMR cluster
already exists, or with the intention of dynamically creating the cluster. What
this implies is that the version of Spark must be dynamic, and be able to
support multiple versions simultaneously. We also store our data on S3, and our Airflow
codebase has this assumption built-in. Ideally we should be able to read from some mock
S3 instance as well.

A loose spec for the service:

* Be able to respond to EMR API requests.
* Be able to batch Spark jobs of any version.
* Submtted Spark jobs should be able to read from a mock instance of S3 and integrate with Airflow.

I started by building on top of [moto][1], which essentially handles the first part of the spec.
Making sure the Spark jobs could talk to a mock S3 instance was surprisingly tricky, and is well covered by 
[this excellent blog post][12]. The last part of the spec is the meat of the problem. Whenever a request to create a cluster
is received _localemr_ forks a process. This new process is responsible for creating a Docker container
with the correct Spark version and submitting Spark jobs to th container it brought up. In order to create this
container the forked process needs to communicate with the [Docker socket][11]. In case _localemr_ is running 
inside a Docker container itself, this typically means exposing the socket via 
`-v /var/run/docker.sock:/var/run/docker.sock`. The container created by _localemr_ is able
to batch and run Spark jobs through [Apache Livy][7]. Due to [LIVY-41][10], a uniqueness constraint is 
enforced on the submitted job names, which is a characteristic not present in EMR. For this reason Livy version 0.5.0 is used. 

This setup relies on having a Docker image with the correct Spark version and Apache Livy in a reachable Docker repository. 
By default, it is configured to look for this image at `davlum/localemr-container`. [Another repo][9] which 
takes advantage of GitHub Actions' [build matrix][8] to iterate over versions of Spark that
are used by EMR, makes such images available in that repository. The Docker repository is configurable by an
environment variable so users can customize the image used.

One of the big issues right now is the time it takes to pull this image the first go-round. After that first
pull the image will be local, but it's definitely a less than ideal experience. The images are
unfortunately large due to bundling Livy and Spark. I'm hoping to improve this by building the imaes with [nix][13].
The [repo][0] itself has some examples that should be fairly easy to run to get things going. Give it a try!

[0]: <https://github.com/davlum/localemr>
[1]: <https://github.com/spulec/moto>
[2]: <https://github.com/localstack/localstack>
[3]: <https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-what-is-emr.html>
[4]: <https://airflow.apache.org/>
[5]: <https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-release-components.html>
[6]: <https://spark.apache.org/>
[7]: <https://livy.apache.org/>
[8]: <https://docs.github.com/en/actions/configuring-and-managing-workflows/configuring-a-workflow#configuring-a-build-matrix>
[9]: <https://github.com/davlum/localemr-container>
[10]: <https://issues.apache.org/jira/browse/LIVY-41>
[11]: <https://docs.docker.com/engine/reference/commandline/dockerd/>
[12]: <https://medium.com/@sumitsu/unit-testing-aws-s3-integrated-scala-spark-components-using-local-s3-mocking-tools-8bb90fd58fa2>
[13]: <https://nixos.org/>