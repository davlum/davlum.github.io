---
title: A Local EMR
subtitle: Trying to improve the development experience
tags: [aws, python, data engineering, ops]
---

Although cloud computing offers many benefits in terms of maintenance, this 
frequently comes at the cost of easily being able to develop locally. This is 
evidenced by tools such as [moto][1] and [openstack][2] which attempt to imitate 
AWS infrastructure. [Localemr](https://github.com/davlum/localemr) is a service 
you can run on your computer that tries to imitate Amazon EMR to help teams 
iterate faster. With this service developers can create 'clusters' and submit Spark 
jobs locally.

I had a lot of fun building this, and in this post I'll cover what motivated it
and how it currently works, although the latter is definitely subject to change.

First and foremost, why is being able to develop locally important? Well in the
extreme scenario, if CI/CD and staging environments are absent and/or useless, 
then the only way to test your code is to test in production. Obviously not 
the ideal scenario. Now what if you do have CI/CD and staging, but no local 
development? CI/CD is not a binary thing, there are varying levels of quality, 
and chances are if you cannot provide development environments, then you probably 
don't have robust integration testing in your CI/CD either as these will frequently
use similar infrastructure. So then you're left with staging. What quickly happens is
that staging becomes the development environment. Developers push to staging not knowing
if things will break or not, only to find out they need to open another PR and use more 
developer time in PR review in order push a fix. This happens over and over as teams
treat staging as dev until eventually staging becomes unusable from breaking so many times,
and we're back to testing in prod. 

The difference between a staging environment and a development environment is that 
development environments are _per user_ (or per PR). The developer doesn't need to inconvenience other
devs with a PR to test their changes, they can make the change and test the results themselves
right away and break the environment as they please. Speaking with a colleague, he mentioned 
a company he had worked at didn't even have staging, as their local development experience was
so complete that staging ended up being more of a hindrance than a help.

Back to EMR. My data engineering team makes heavy use of [Amazon EMR][3]. From the site:

> Amazon EMR is a managed cluster platform that simplifies running big data frameworks.


Our use case is mainly to submit [Spark][6] jobs. When a cluster is created, it is
given a [release label][5] which determines the version of Spark used. We schedule
these Spark jobs using [Airflow][4] with the assumption that a long running EMR cluster
already exists, or with the intention of dynamically creating the cluster. What
this implies is that the version of Spark must be dynamic, and be able to
support multiple versions simultaneously. We also store our data on S3, and our Airflow
codebase has this assumption built-in. Ideally we should be able to read from some mock
S3 instance as well.

A loose spec for the service:

* Be able to respond to EMR API requests
* Be able to batch Spark jobs of any version.
* Be able to read from a mock instance of S3

I started by building on top of [moto][1]. Moto essentially handles the first part of the spec for me.
Now the next part of the spec which is the meat of the problem. 







[1]: <https://github.com/spulec/moto>
[2]: <https://github.com/localstack/localstack>
[3]: <https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-what-is-emr.html>
[4]: <https://airflow.apache.org/>
[5]: <https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-release-components.html>
[6]: <https://spark.apache.org/>
